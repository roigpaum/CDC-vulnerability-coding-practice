# Import the data into R (go to where downloaded then move to new project)
library(readr)
library(purrr)
library(dplyr)

#Read data into R
svi_data <- read_csv("data/cdc_svi_oregon_county_subset.csv")

#Identify metric of interest and store in function
#You can also say if it's good or bad, meaning you can add options to functions
metrics <- c("EP_POV", "EP_DISABL", "EP_MINRTY", "EP_AGE65", "EP_LIMENG", "EP_CROWD", "EP_SNGPNT")
high_is_vulnerable <- c(TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE)
svi_data[metrics]

#Standardize metrics for comparison
#Make rank(svi_data$EP_DISABL) / length(svi_data$EP_DISABL) into generic function like my_fun
#Added optional parameter for function

standardize <- function(input, flip=FALSE) {
  output <- return(rank(input)/length(input))
  if (flip) {
    output <- 1-output
  }
  return(output)
}

#For one value
map_dfc(svi_data[metrics], standardize)

#For 2 values, now where low is less vulnerable, and high is more vulnerable. ! makes the reciprocal
standardized_data <- map2_dfc(svi_data[metrics],! high_is_vulnerable, standardize)
standardized_data <- bind_cols(svi_data["COUNTY"], standardized_data)

#Consolidate metrics into one 
standardized_data$summary <- rowSums(standardized_data[metrics])

#Sort results by vulnerability
standardized_data <- standardized_data[order(standardized_data$summary),]

#Save the results and you can look at it in spreadsheet
write.csv(standardized_data, file = "county_rankings.csv")
